---
title: "CANN-Models Applied to a Belgian MTPL Dataset"
author: "Bruno Deprez"
date: "`r Sys.Date()`"
output: html_document
bibliography: Bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

With the growth of available data and evolution of (cloud) computing, their is an increased interest to apply machine learning techniques to actuarial problems. This trend has been accelerated due to the Covid-19 crisis, see [@eiopa2019big] and [@eiopa2021XAI].

In this paper, we take a look at one way of introducing neural networks when working with a motor third party liability (hereinafter "MTPL") insurance, more specifically when modelling the claim count of an affiliate. We will see that naively training a neural network misses some of the key aspects of the data. When dealing with claim count data, most values are 0 since most policy holders do not report a claim during the year. However, the data is also skewed with fat tails. This makes it harder to train a neural network on the data, even after normalisation. 

The solution presented in this text comes from [@schelldorfer2019nesting]. The paper deals with Combined Actuarial Neural Networks (hereinafter "CANN"), which starts from a classical statistical model, like a generalised linear model (hereinafter "GLM"), and uses the neural network to uncover interaction effects and make adjustments to the predictions from the GLM.

Where [@schelldorfer2019nesting] makes mostly ad hoc choices, since the focus was more on showing the reader how to implement a CANN model, we go a step further and also do some basic hyperparameter tuning. 

We start with a small introduction of the dataset and show visually why we need to move from a neural network to a CANN model. After that, we start modelling a full CANN using the MTPL dataset for Belgium. The results of different models will be compared with each other in order to make some conclusions on this new technique. 

All neural network implementations are done using the `keras` library in `R`.

Please note that this is mostly a project done in the author's free time, so the code is not always optimised and some *best practices* may be absent. 

# A First Example
## The Data Set
Before starting to program, we need to load the necessary libraries and the MTPL dataset. 
```{r set_up, message=FALSE}
#### Setup ####

# Libraries needed
library(tidyverse)
library(readxl)
library(MASS)
library(readxl)
library(maidrr)
library(sp)
library(sf)
library(mapview)
library(mgcv)

library(keras)

# Import all data needed
mtpl_be <- maidrr::mtpl_be #the MTPL data set used here
postal_codes <- read_excel("../inspost.xls")

belgium_shape_sf <- st_read('../shape file Belgie postcodes/npc96_region_Project1.shp', quiet = TRUE)
belgium_shape_sf <- st_transform(belgium_shape_sf, CRS("+proj=longlat +datum=WGS84")) 
```

The dataset used is included in the `maidrr` package (see <https://github.com/henckr/maidrr>). It contains 163210 entries, each having 12 features (plus 1 column with an uninformative id). An important one is the $expo$ column, which represents the duration of the contract (exposure) during our observation period. Since not all contracts were in force for the whole observation year, it is important to incorporate this into our modelling. 

As an example suppose that we have two contracts where both policy holders reported one claim by the end of the year. The first one was in force over the whole year, but the second was only underwritten one month ago. There is clearly a difference in risk between the two, since we cannot say that one claim over a year and one claim over a month are the same. Hence, it will be crucial to use the exposure when modelling the number of claims for a given policy holder. 

A summary of the other features are given below. There is a mix of both numerical and categorical variables. The interesting reader can exploratory data analysis themselves. 
```{r str}
str(mtpl_be)
```
The only thing that we still mention is the fact that the postal code does not correspond to a specific municipality, but the postal code areas (first two digits of the four digit postal code), see the map below taken from <https://nl.wikipedia.org/wiki/Postnummers_in_Belgi%C3%AB>.

```{r, fig.width = 4, echo = FALSE}
knitr::include_graphics("2_digit_postcode_belgique.png")
```

## Modelling Claim Count Using Age
### A First Try
To illustrate the inherent flaw of a neural network when dealing with actuarial problems, we begin with a small model where the age of the policy holder is used to determine the risk, i.e., the expected number of claims^[This example is inspired by <https://katrienantonio.github.io/hands-on-machine-learning-R-module-3/sheets/ML_part3.html#94>]. Since we are dealing with claim counts, we assume that it follows a Poisson distribution. This distributional assumption is used to apply a generalised additive model (hereinafter "GAM") to model the expected number of claims via:
$$ \mathbb{E}(Y_i) = d_i\cdot\exp\left(\beta_0 + f_1(\text{age}_i)\right) $$
Note that we use the exponential function to insure positivity of the mean, and that we have also incorporated the exposure, denoted by $d_i$. The function $f_1$ is a *smoother*, which in our case is equal to a thin plate spline. A full understanding of this is not necessary to follow the rest of the text. 

The result of this GAM, which will be used as our base line model, is compared to the neural network. In the code, we incorporate the exposure as an offset, since we can rewrite the previous formula as follows:
$$ \mathbb{E}(Y_i) = \exp\left(\log(d_i)+\beta_0 + f_1(\text{age}_i)\right) $$
For the neural network, we use two hidden layers, each having 5 neurons. The activation function is the `tanh` in both cases. The exposure is added using a the `LogExpo` input node which has a non-trainable weight equal to 1 and does not use a bias term when connecting with the `layer_add` (exposure is fixed and should not be tampered with). As for the GAM, we use the exponential function as the final activation function when feeding into the output node, and this last connection is also non-trainable. The loss function is `poisson` since we assume a Poisson distribution.

```{r GAMage}
#GAM
GAM_age <- gam(nclaims ~ s(ageph), 
               data = mtpl_be, 
               offset = log(expo),
               family = poisson(link = "log"))

```

```{r NNage}
#NN
set.seed(1997)
Design <- layer_input(shape = c(1), dtype = "float32", name = "Design")
LogExpo <- layer_input(shape = c(1), dtype = "float32", name = "LogExpo")

Network <- Design %>%
  layer_batch_normalization(input_shape = c(1)) %>%
  layer_dense(units = 5, activation = 'tanh', name = 'hidden1') %>%
  layer_dense(units = 5, activation = 'tanh', name = 'hidden2') %>%
  layer_dense(units = 1, activation = 'linear', name = 'Network')

Response <- list(Network, LogExpo) %>%
  layer_add(name = 'Add') %>%
  layer_dense(units = 1, activation = k_exp, name = 'Response', trainable = FALSE,
              weights = list(array(1, dim=c(1,1)), array(0, dim = c(1))))

NN_age <- keras_model(inputs = c(Design, LogExpo), outputs = c(Response))

NN_age %>% compile(optimizer = optimizer_adam(), 
                    loss = "poisson")

Xfeat <- as.matrix(mtpl_be$ageph)
Xexpo <- as.matrix(log(mtpl_be$expo))
Ylearn <- as.matrix(mtpl_be$nclaims)

fit_nn_age <- NN_age %>% fit(list(Xfeat, Xexpo), Ylearn, 
                         epochs = 10, 
                         batch_size = 1718,
                         validation_split = 0.2, 
                         verbose = 0)

```

In order to compare the outcomes, we look at the predictions for every age. In addition, we also considered each age as an individual factor an fit a GLM (plotted in black). When looking at the plot below, we see that the GAM captured the trend in the data well (plotted in blue). We see a decrease of claim frequency for increasing ages. This decrease stabalises around 40 to 50 years old, most likely since the children of these people learn to drive with their parent's car, and cause more accidents. At older ages, we see that the number of accidents increases again. The GLM, with each age its own factor, fluctuates sustantially at the end. This is most likely cause by the fact that we have fewer observations for older ages.

The neural network (plotted in red), on the other hand, is able to point to the fact that claim frequency improves with age, but it may be too *simplistic*. The patterns throughout the ages is not captured.

Note that the plot shows the *yearly* average claim count predicted by the model, since we say that for each age we consider, the exposure is equal to 1 (year). We did not include the age of 18 in the plot, since the prediction for that factor in the GLM were very large. The underlying pattern for the other ages were not well visible due to the large scale needed on the y-axis. 

```{r plot_nn_gam_glm}
factor_age <- glm(nclaims ~ factor(ageph)-1, 
                  offset = log(expo), 
                  data = mtpl_be, 
                  family = poisson(link="log"))

XTest <- (min(mtpl_be$ageph)+1):max(mtpl_be$ageph)
ExpoTest <- rep(1, length(XTest)) #only yearly average claim count

y_fit_factor <- predict(factor_age, 
                        newdata = data.frame(ageph = XTest, expo = 1), 
                        type = "response")

y_fit_NN <- predict(NN_age, 
                    list(XTest, log(ExpoTest)))

y_fit_GAM <- predict(GAM_age, 
                     newdata = data.frame(ageph = XTest, expo = 1), 
                     type = "response")

Comparison <- tibble(Age = XTest, 
                     GAM = y_fit_GAM, 
                     NN = y_fit_NN, 
                     factors = y_fit_factor)

ggplot(Comparison, aes(x = Age)) +
  geom_line(aes(y= GAM), color = "blue") +
  geom_line(aes(y= NN), color = "red") +
  geom_line(aes(y= factors))+
  ylab("Response") +
  ggtitle("Difference between GAM and Neural Network")+
  theme_bw()

```

This clearly illustrates that a classic neural network may not be best suited to model these problems. In the above example, the network was rather small. It could be possible that a larger network with more hidden layers and nodes in each layer will better capture the patterns in the data. One need to keep in mind, however, that this is not practical. A very large model takes much more time to fit and is harder to maintain, and this does not weight against the marginal gains that can be made compared to the already satisfactory results coming from the GAM. A simpler model makes it also easier to implement and use in practice, and it has the advantage that one can more easily play with the parameters to express a certain business goal set up by the insurance company, e.g., special discounts for certain groups^[This point will not be solved by moving to CANN models. This is better done by using a smart way of dividing all explanatory variables into factor variables. These are then used to model a GLM, like we did, with each factor variable having its own coefficient. For more details, we refer to [@henckaerts2021boosting]]. 

### Moving to CANN's
In order to solve some of the shortcomings, one can move to the Combined Actuarial Neural Network setting, as represented in [@schelldorfer2019nesting]. The idea here is to use the predictions coming from the more classical statistical models, like the GLM and GAM, and use them as a guide for the your network. This way, the network itself does not model the Poisson distribution itself, but the smaller patterns in the data, not picked up by the statistical model. This is done by introducing a so-called (non-trainable) skip connection to the neural network, illustrated below (source: [@schelldorfer2019nesting]). 

```{r, out.width="40%", echo = FALSE}
knitr::include_graphics("Skip.png")
```

This skip connection needs to be cleverly integrated into our network. We still assume the exponential activation in the final step in the network (to insure positivity of the predicted mean). The output of a classic neural network would then look like this:
$$ \text{output} = \exp\left( \sum_i n_i w_i + b \right), $$
where $n_i$ are the inputs from the previous layer, $w_i$ the weights corresponding to the connections, and $b$ is the bias term. In a previous step, we have taken the exposure into account like this:
$$ \text{output} = \exp\left( \sum_i n_i w_i + b + \log(\text{expo}) \right). $$
The trick is now to use the skip connection as a first base prediction, and incorporate this into the output as well. We do this again by using the logarithm. We get the following:
$$ \text{output} = \exp\left( \sum_i n_i w_i + b + \log(\text{expo}) + \log(\text{base}) \right) = \text{expo}\cdot\text{base}\cdot\exp\left( \sum_i n_i w_i + b \right). $$
How to do this in `R` is illustrated below. We recycle the GAM from before to use as our base model. Note that we do **not** include `type = "response"` in the predict function. This way, we already have the logarithm of the predicted average claims. We also add the logarithm of the exposure, like in the equation above.
```{r CANN}
#CANN
set.seed(1997)
Vlearn2 <- as.matrix(predict(GAM_age) + log(mtpl_be$expo)) #You include exposure
LogGAM <- layer_input(shape = c(1),   dtype = 'float32', name = 'LogGAM')

Design2 <- layer_input(shape = c(1), dtype = "float32", name = "Design2")

Network2 <- Design2 %>%
  layer_batch_normalization() %>%
  layer_dense(units = 5, activation = 'tanh', name = 'hidden2_1') %>%
  layer_dense(units = 1, activation = 'linear', name = 'Network2')

Response2 <- list(Network2, LogGAM) %>%
  layer_add(name= "Add2") %>%
  layer_dense(units=1, activation="exponential", name = 'Response2', trainable=FALSE,
              weights=list(array(1, dim=c(1,1)), array(0, dim=c(1))))

CANN_age <- keras_model(inputs = c(Design2, LogGAM), 
                        outputs = c(Response2))

CANN_age %>% compile(optimizer = optimizer_rmsprop(), 
                     loss = "poisson")

CANN_age%>% fit(list("Design2" = Xfeat, "LogGAM"=Vlearn2), Ylearn, 
                                epochs = 25, 
                                batch_size = 1718, 
                                verbose = 0)
```

The plot from before then becomes:
```{r CANN_Base}
XTest <- (min(mtpl_be$ageph)+1):max(mtpl_be$ageph)
ExpoTest <- rep(1, length(XTest))

y_fit_factor <- predict(factor_age, newdata = data.frame(ageph = XTest, expo = 1), 
                        type = "response")

y_fit_GAM <- predict(GAM_age, newdata = data.frame(ageph = XTest, expo = 1), 
                     type = "response")

y_fit_CANN <- predict(CANN_age, list(XTest, 
                                     predict(GAM_age, 
                                                    newdata = data.frame(ageph = XTest, expo = 1))))

Comparison <- tibble(Age = XTest, 
                     GAM = y_fit_GAM, 
                     CANN = y_fit_CANN, 
                     factors = y_fit_factor)

ggplot(Comparison, aes(x = Age)) +
  geom_line(aes(y= GAM), color = "blue") +
  geom_line(aes(y= CANN), color = "darkgreen")+
  geom_line(aes(y= factors))+
  ylab("Response") +
  ggtitle("Difference between GAM and CANN")+
  theme_bw()

```

It may not be highly visible here, but the CANN has made some correction to the GAM. Here, we see that this is rather small. The true power will become clear later in this document, when we model the claim count with all variables. Using a neural network like this will allow us to incorporate interactions between the different variables that are not captured by the GAM. This makes it also much easier to incorporate these interactions. You can define interactions when fitting the GAM, but then you need to specify each one explicitly, which can take a lot of trial and error to incorporate the right ones, and only the right ones. In addition, the neural network captures interactions that may not be of a multiplicative nature, which is much harder to do with a GAM or GLM. Let's see all this in practice in the next section. 

# Modelling the Full Dataset
## Some theory


# References
